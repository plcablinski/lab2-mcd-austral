{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FUENTES**:\n",
    "\n",
    "PetFinder Kaggle:\n",
    "\n",
    "https://www.kaggle.com/competitions/petfinder-adoption-prediction/data\n",
    "\n",
    "First Tutorial:\n",
    "\n",
    "https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5\n",
    "\n",
    "Second Deep Tutorial:\n",
    "\n",
    "https://rumn.medium.com/part-1-ultimate-guide-to-fine-tuning-in-pytorch-pre-trained-model-and-its-configuration-8990194b71e\n",
    "\n",
    "Logo Recognition API:\n",
    "\n",
    "https://heartbeat.comet.ml/logo-recognition-ios-application-using-machine-learning-and-flask-api-aec4eff3be11\n",
    "\n",
    "Hybrid (multimodal) neural network architecture : Combination of tabular, textual and image inputs:\n",
    "\n",
    "https://medium.com/@dave.cote.msc/hybrid-multimodal-neural-network-architecture-combination-of-tabular-textual-and-image-inputs-7460a4f82a2e\n",
    "\n",
    "\n",
    "\n",
    "### **INDICACIONES PREVIAS**:\n",
    "\n",
    "+ **Git**:\n",
    "    + Clonamos el repo: root de todos los repos y ponemos git clone \"url_repo\"\n",
    "    + Hacemos el checkout de la rama main: git checkout -b new-branch\n",
    "\n",
    "+ **Poetry**:\n",
    "    + Instalamos poetry: https://python-poetry.org/docs/\n",
    "    + Realizamos un Update del pyproject: poetry update\n",
    "    + Activamos el entorno que creo poetry: poetry shell\n",
    "    + Intentamos correr una celda, si nos pide seleccionar el environment y no lo vemos en la lista, cerrar y volver abrir VSC\n",
    "\n",
    "+ **Torch y CUDA**:\n",
    "    + Verificar que versión pide torch:\n",
    "        + Versión de torch instalada: poetry show (en mi caso la 1.13.1)\n",
    "        + Buscar la versión correspondiente en la documentación: https://pytorch.org/get-started/previous-versions/  (en mi caso el 11.7)\n",
    "    + Instalar CUDA para Torch (buscar la versión correspondiente de CUDA): https://developer.nvidia.com/cuda-11-7-0-download-archive\n",
    "    + Verificar que CUDA esté funcional: correr en una celda torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\anaconda3\\envs\\ldi2_cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#import cv2\n",
    "#from PIL import Image\n",
    "#from pathlib import Path\n",
    "\n",
    "import optuna\n",
    "from optuna.artifacts import FileSystemArtifactStore, upload_artifact\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from joblib import load, dump\n",
    "\n",
    "from utils import plot_confusion_matrix\n",
    "# Verificamos que CUDA está funcional\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seteo el Modelo**\n",
    "\n",
    "Teoría de Resnet: https://towardsdatascience.com/introduction-to-resnets-c0a830a288a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo modelo ResNet entrenado en Imagenet\n",
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "# Modificar la última capa para adaptarse a tu problema específico\n",
    "num_ftrs = resnet50.fc.in_features\n",
    "resnet50.fc = torch.nn.Linear(num_ftrs, 5) # Clasificación 5 clases\n",
    "# Configuro para usar cuda si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet50 = resnet50.to(device)\n",
    "# Instancio del criterio de pérdida CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Instancio Stochastic Gradient Descent (SGD): Defino el parámetro del Learning Rate (define \"el paso\" en que avanzan los pesos en cada iteración) y el Momentum (pone innercia a la dirección del gradiente descendiente para que no cambie de dirección en minimos locales)\n",
    "optimizer = optim.SGD(resnet50.parameters(), lr=0.001, momentum=0.9) # Parámetros default del SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seteo parámetros, directorios y funciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_DIR = 'C:/'\n",
    "PATH_TO_TRAIN = os.path.join(BASE_DIR, \"input/petfinder-adoption-prediction/train/train.csv\")\n",
    "PATH_TO_IMAGES_DIR = os.path.join(BASE_DIR, \"input/petfinder-adoption-prediction/train_images\")\n",
    "PATH_TO_TEMP_FILES = os.path.join(BASE_DIR, \"UA_MDM_LDI_II/work/optuna_temp_artifacts\")\n",
    "PATH_TO_OPTUNA_ARTIFACTS = os.path.join(BASE_DIR, \"UA_MDM_LDI_II/work/optuna_artifacts\")\n",
    "\n",
    "MODEL_NAME = '04 ResNet'\n",
    "\n",
    "MODEL_VERSION = '1.0.0'\n",
    "\n",
    "# Parametros y variables\n",
    "CREATE_PYTORCH_DIRECTORIES = 1\n",
    "SEED = 42\n",
    "BATCH_SIZE = 15\n",
    "TEST_SIZE = 0.2\n",
    "IMAGE_SIZE = 299\n",
    "CPU_CORES = os.cpu_count()\n",
    "\n",
    "# Armo el nuevo directorio de train\n",
    "new_train_directory = os.path.join(BASE_DIR, 'UA_MDM_LDI_II/work/train_images_classes')\n",
    "os.makedirs(new_train_directory, exist_ok=True) # si ya existe el nombre, lo deja como está\n",
    "\n",
    "# Armo el nuevo directorio de validación\n",
    "new_val_directory = os.path.join(BASE_DIR, 'UA_MDM_LDI_II/work/val_images_classes')\n",
    "os.makedirs(new_val_directory, exist_ok=True)\n",
    "\n",
    "# Definir las clases ordenadas\n",
    "class_names = ['0', '1', '2', '3', '4']\n",
    "\n",
    "# Mapear las etiquetas de las clases a números enteros consecutivos\n",
    "class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "# Creo las carpetas de clases dentro de los directorios\n",
    "for clase in class_names: # Una para cada clase\n",
    "   os.makedirs(os.path.join(new_train_directory, str(clase)), exist_ok=True)\n",
    "   os.makedirs(os.path.join(new_val_directory, str(clase)), exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Funciones para la carga y el preproceso\n",
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    # Calcula el factor de escala necesario para redimensionar la imagen de manera que el lado más largo tenga el tamaño deseado \n",
    "    ratio = float(IMAGE_SIZE)/max(old_size)\n",
    "    # Calcula las nuevas dimensiones de la imagen \n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # Redimensiona la imagen con el nuevo tamaño\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    # Calcula las diferencias de tamaño y agrega pixeles (color negro) en los extremos para que quede centrada y cuadrada \n",
    "    delta_w = IMAGE_SIZE - new_size[1]\n",
    "    delta_h = IMAGE_SIZE - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_image = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def load_image(pet_id):\n",
    "    path_to_image = os.path.join(PATH_TO_IMAGES_DIR, f'{pet_id}-1.jpg') # Irá a la primera imagen de la mascota\n",
    "    image = cv2.imread(path_to_image)\n",
    "    # Convierte la imagen de BGR a RGB porque estos modelos esperan ese orden de canales\n",
    "    image = cv2.convertScaleAbs(image)\n",
    "    image= cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    new_image = resize_to_square(image)\n",
    "    return new_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_pet(pet_id):\n",
    "    path_to_image = os.path.join(PATH_TO_IMAGES_DIR, f'{pet_id}-1.jpg') # Irá a la primera imagen de la mascota\n",
    "    # Cargar la imagen\n",
    "    image_to_show = cv2.imread(path_to_image)\n",
    "    # Convertir a formato RGB\n",
    "    image_to_show = cv2.cvtColor(image_to_show, cv2.COLOR_BGR2RGB)\n",
    "    # Visualizar la imagen\n",
    "    plt.imshow(image_to_show)\n",
    "    plt.axis('off')  # No mostrar los ejes\n",
    "    plt.show()\n",
    "\n",
    "def visualize_image(image):\n",
    "    # Convierte la imagen a un formato de enteros (CV_8U)\n",
    "    image = cv2.convertScaleAbs(image)\n",
    "    image= cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Visualizar la imagen\n",
    "    plt.imshow(image.astype(np.uint8))\n",
    "    plt.axis('off')  # No mostrar los ejes\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cargo y Proceso Data**\n",
    "\n",
    "Nota: Pytorch necesita que estén las imágenes en los distintos directorios según su clase y su participación en el training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo\n",
    "train_df = pd.read_csv(PATH_TO_TRAIN)\n",
    "\n",
    "# Split para validación\n",
    "train_data, val_data = train_test_split(train_df,\n",
    "                               test_size = TEST_SIZE,\n",
    "                               random_state = SEED,\n",
    "                               stratify = train_df.AdoptionSpeed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if CREATE_PYTORCH_DIRECTORIES == 0: # Poner en 0 si ya tengo las carpetas train_images_classes y val_images_classes con las imágenes copiadas\n",
    "    # Función para copiar las imágenes a los directorios correspondientes\n",
    "    def copy_imag(data, directorio_destino):\n",
    "        for index, row in data.iterrows():\n",
    "            petID = row['PetID']\n",
    "            adoption_speed = row['AdoptionSpeed']\n",
    "            \n",
    "            # Nombre del archivo de imagen\n",
    "            nombre_archivo = f\"{petID}-1.jpg\"\n",
    "            \n",
    "            # Ruta completa de la imagen de origen\n",
    "            ruta_origen = os.path.join(PATH_TO_IMAGES_DIR, nombre_archivo)\n",
    "            \n",
    "            # Ruta completa del directorio de destino\n",
    "            ruta_destino = os.path.join(directorio_destino, str(adoption_speed), nombre_archivo)\n",
    "            \n",
    "            # Verificar si el archivo de origen existe\n",
    "            if os.path.exists(ruta_origen):\n",
    "                # Copiar el archivo de origen al directorio de destino\n",
    "                shutil.copy2(ruta_origen, ruta_destino)\n",
    "        print(\"Completada la copia a: \",str(directorio_destino))\n",
    "\n",
    "    # Copiar las imágenes al directorio de train\n",
    "    copy_imag(train_data, new_train_directory)\n",
    "\n",
    "    # Copiar las imágenes al directorio de val\n",
    "    copy_imag(val_data, new_val_directory)\n",
    "\n",
    "    print(\"Proceso completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero los DataLoaders\n",
    "def create_dataloaders(train_directory, val_directory, batch_size, num_workers):\n",
    "    # Transformaciones de imagen para el conjunto de entrenamiento\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Transformaciones de imagen para el conjunto de validación (sin data augment)\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Crear conjuntos de datos para el conjunto de entrenamiento y validación\n",
    "    conjunto_entrenamiento = datasets.ImageFolder(train_directory, transform=train_transforms)\n",
    "    conjunto_validacion = datasets.ImageFolder(val_directory, transform=val_transforms)\n",
    "\n",
    "    # Asignar las clases ordenadas al conjunto de datos\n",
    "    conjunto_entrenamiento.class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "    conjunto_validacion.class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "    # Crear dataloaders para el conjunto de entrenamiento y validación\n",
    "    train_dataloader = torch.utils.data.DataLoader(conjunto_entrenamiento, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_dataloader = torch.utils.data.DataLoader(conjunto_validacion, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "# Aplico las funcion de los DataLoaders\n",
    "train_dataloader, val_dataloader = create_dataloaders(new_train_directory , new_val_directory , BATCH_SIZE, CPU_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero una lista de PetIDs con imagen en el orden en que aparecen en el data loader\n",
    "test_sample_ids = [i[0].split('/')[-1].split('-')[0] for i in val_dataloader.dataset.samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entreno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: the launch timed out and was terminated\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 133\u001b[0m\n\u001b[0;32m    129\u001b[0m         upload_artifact(trial, model_path, artifact_store)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model,best_kappa\n\u001b[1;32m--> 133\u001b[0m best_model,_ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet50\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Guardo el modelo\u001b[39;00m\n\u001b[0;32m    140\u001b[0m run_id \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m, in \u001b[0;36mtrain_val\u001b[1;34m(model, criterion, optimizer, dataloaders, datasets, device, num_epochs, lr, momentum, trial)\u001b[0m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(resnet50\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39mmomentum) \u001b[38;5;66;03m# Parámetros default del SGD\u001b[39;00m\n\u001b[0;32m      6\u001b[0m since \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m best_model_wts \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     10\u001b[0m best_kappa \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m999\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Pablo\\anaconda3\\envs\\ldi2_cuda\\Lib\\copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[1;32mc:\\Users\\Pablo\\anaconda3\\envs\\ldi2_cuda\\Lib\\copy.py:285\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[0;32m    284\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[1;32m--> 285\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Pablo\\anaconda3\\envs\\ldi2_cuda\\Lib\\copy.py:143\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    141\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pablo\\anaconda3\\envs\\ldi2_cuda\\Lib\\site-packages\\torch\\_tensor.py:122\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    114\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default implementation of __deepcopy__() for wrapper subclasses \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly works for subclass types that implement clone() and for which \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferent type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         )\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     new_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_typed_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_quantized:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;66;03m# quantizer_params can be different type based on torch attribute\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         quantizer_params: Union[\n\u001b[0;32m    126\u001b[0m             Tuple[torch\u001b[38;5;241m.\u001b[39mqscheme, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    127\u001b[0m             Tuple[torch\u001b[38;5;241m.\u001b[39mqscheme, Tensor, Tensor, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    128\u001b[0m         ]\n",
      "File \u001b[1;32mc:\\Users\\Pablo\\anaconda3\\envs\\ldi2_cuda\\Lib\\site-packages\\torch\\storage.py:853\u001b[0m, in \u001b[0;36mTypedStorage._deepcopy\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_deepcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, memo):\n\u001b[1;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_wrapped_storage(\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_untyped_storage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Pablo\\anaconda3\\envs\\ldi2_cuda\\Lib\\copy.py:143\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    141\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pablo\\anaconda3\\envs\\ldi2_cuda\\Lib\\site-packages\\torch\\storage.py:114\u001b[0m, in \u001b[0;36m_StorageBase.__deepcopy__\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cdata \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cdata]\n\u001b[1;32m--> 114\u001b[0m new_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m memo[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cdata] \u001b[38;5;241m=\u001b[39m new_storage\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_storage\n",
      "File \u001b[1;32mc:\\Users\\Pablo\\anaconda3\\envs\\ldi2_cuda\\Lib\\site-packages\\torch\\storage.py:128\u001b[0m, in \u001b[0;36m_StorageBase.clone\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclone\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a copy of this storage.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnbytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: the launch timed out and was terminated\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "def train_val(model, criterion, optimizer, dataloaders, datasets, device, num_epochs=20, lr=0.001, momentum = 0.9 ,trial=None):\n",
    "    \n",
    "    # Instancio Stochastic Gradient Descent (SGD): Defino el parámetro del Learning Rate (define \"el paso\" en que avanzan los pesos en cada iteración) y el Momentum (pone innercia a la dirección del gradiente descendiente para que no cambie de dirección en minimos locales)\n",
    "    optimizer = optim.SGD(resnet50.parameters(), lr=lr, momentum=momentum) # Parámetros default del SGD\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_kappa =  -999\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    try:\n",
    "        previous_best = study.best_value\n",
    "    except:\n",
    "        previous_best = -999\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        kappa_labels_true = []\n",
    "        kappa_labels_predicted = []\n",
    "        output_scores = []\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    elif phase == 'val':\n",
    "                        kappa_labels_true.extend(labels.cpu().numpy().tolist())\n",
    "                        kappa_labels_predicted.extend(preds.cpu().numpy().tolist())\n",
    "                        outputs_np = outputs.cpu().numpy()\n",
    "                        output_scores.extend([outputs_np[i,:] for i in range(outputs_np.shape[0])])\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                #END OF BATCH\n",
    "\n",
    "            epoch_loss = running_loss / len(datasets[phase])\n",
    "            epoch_acc = running_corrects.double() / len(datasets[phase])\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "                kappa_score = np.nan\n",
    "            else:\n",
    "                val_losses.append(epoch_loss)\n",
    "                kappa_score = cohen_kappa_score(kappa_labels_true,\n",
    "                                  kappa_labels_predicted,\n",
    "                                  weights = 'quadratic')\n",
    "                    \n",
    "\n",
    "\n",
    "            print(f'{phase.title()} Loss: {epoch_loss:.4f} Acc: {epoch_acc*100:.2f}% Kappa: {kappa_score:.3f}')\n",
    "\n",
    "            # If this is the best Epoch so far -> Deep copy the model\n",
    "            if phase == 'val' and kappa_score > best_kappa:\n",
    "                best_acc = epoch_acc\n",
    "                best_kappa = kappa_score\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "                #Best Epoch within a trial and better than previous trials\n",
    "                if trial is not None and best_kappa > previous_best:\n",
    "\n",
    "                    #Save test dataset with predictions\n",
    "                    predicted_filename = os.path.join(PATH_TO_TEMP_FILES,f'test_{trial.study.study_name}_{trial.number}.joblib')\n",
    "                    predicted_df = pd.DataFrame({'PetID':test_sample_ids,\n",
    "                                'pred':output_scores}).merge(val_data, on='PetID')\n",
    "                    dump(predicted_df, predicted_filename)\n",
    "\n",
    "                    #Generate and save CM \n",
    "                    cm_filename = os.path.join(PATH_TO_TEMP_FILES,f'cm_{trial.study.study_name}_{trial.number}.jpg')\n",
    "                    plot_confusion_matrix(kappa_labels_true,kappa_labels_predicted).write_image(cm_filename)\n",
    "\n",
    "            #END OF PHASE\n",
    "\n",
    "        #END OF EPOCH\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.2f}%'.format(best_acc * 100))\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Save in optuna trial the best test dataset, cm and model weights\n",
    "    if trial is not None and best_kappa > previous_best:\n",
    "        upload_artifact(trial, predicted_filename, artifact_store)   \n",
    "\n",
    "        upload_artifact(trial, cm_filename, artifact_store)\n",
    "\n",
    "        file_name = f'{MODEL_NAME}_{MODEL_VERSION}_{trial.number}.pth'\n",
    "        model_path = os.path.join(PATH_TO_TEMP_FILES, file_name)\n",
    "        torch.save(model, model_path) # Podemos guardar solo los pesos si queremos: best_model.state_dict()\n",
    "        upload_artifact(trial, model_path, artifact_store)\n",
    "\n",
    "    return model,best_kappa\n",
    "\n",
    "best_model,_ = train_val(resnet50, criterion, optimizer,\n",
    "                       dataloaders={'train': train_dataloader, \n",
    "                                    'val': val_dataloader}, \n",
    "                       datasets={'train': train_data, 'val': val_data}, \n",
    "                       device=device, \n",
    "                       num_epochs=10)\n",
    "# Guardo el modelo\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file_name = f'{MODEL_NAME}_{MODEL_VERSION}_{run_id}.pth'\n",
    "model_path = os.path.join(PATH_TO_TEMP_FILES, file_name)\n",
    "torch.save(best_model, model_path) # Podemos guardar solo los pesos si queremos: best_model.state_dict()\n",
    "print(f'Modelo guardado en {model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1691189/3870832202.py:1: ExperimentalWarning: FileSystemArtifactStore is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "  artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n"
     ]
    }
   ],
   "source": [
    "artifact_store = FileSystemArtifactStore(base_path=PATH_TO_OPTUNA_ARTIFACTS)\n",
    "\n",
    "\n",
    "def optuna_train(trial):\n",
    "\n",
    "    epochs = trial.suggest_int('epochs', 5, 5)\n",
    "\n",
    "    lr = trial.suggest_float('lr', 0.00001, 0.1, log=True)\n",
    "\n",
    "    momentum = trial.suggest_float('momentum', 0.0, 0.95)\n",
    "\n",
    "    _,best_score = train_val(resnet50, criterion, optimizer,\n",
    "                       dataloaders={'train': train_dataloader, \n",
    "                                    'val': val_dataloader}, \n",
    "                       datasets={'train': train_data, 'val': val_data}, \n",
    "                       device=device, \n",
    "                       num_epochs=epochs,\n",
    "                       lr=lr,\n",
    "                       momentum = momentum,\n",
    "                       trial=trial)\n",
    "\n",
    "\n",
    "    return(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-19 10:33:28,576] A new study created in RDB with name: 04 ResNet_1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2906 Acc: 41.16% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3629 Acc: 35.28% Kappa: 0.301\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2824 Acc: 42.34% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3617 Acc: 35.21% Kappa: 0.297\n",
      "Epoch 2/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2764 Acc: 42.43% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3614 Acc: 35.58% Kappa: 0.310\n",
      "Epoch 3/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2699 Acc: 42.48% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3621 Acc: 35.31% Kappa: 0.306\n",
      "Epoch 4/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2622 Acc: 43.46% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.38it/s]\n",
      "/tmp/ipykernel_1691189/4223688077.py:122: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n",
      "/tmp/ipykernel_1691189/4223688077.py:124: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3622 Acc: 35.55% Kappa: 0.310\n",
      "Training complete in 4m 59s\n",
      "Best val Acc: 35.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1691189/4223688077.py:129: ExperimentalWarning:\n",
      "\n",
      "upload_artifact is experimental (supported from v3.3.0). The interface can change in the future.\n",
      "\n",
      "[I 2024-06-19 10:38:27,725] Trial 0 finished with value: 0.3097383240156475 and parameters: {'epochs': 5, 'lr': 0.002011262337616341, 'momentum': 0.10902563525065864}. Best is trial 0 with value: 0.3097383240156475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3021 Acc: 39.78% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4007 Acc: 34.68% Kappa: 0.270\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2098 Acc: 46.27% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.4765 Acc: 30.28% Kappa: 0.230\n",
      "Epoch 2/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0743 Acc: 53.63% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.5226 Acc: 30.11% Kappa: 0.177\n",
      "Epoch 3/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9007 Acc: 63.01% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.6410 Acc: 30.64% Kappa: 0.203\n",
      "Epoch 4/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6893 Acc: 72.56% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.50it/s]\n",
      "[I 2024-06-19 10:43:26,045] Trial 1 finished with value: 0.2704338655533711 and parameters: {'epochs': 5, 'lr': 0.04251926496776635, 'momentum': 0.014269978621969614}. Best is trial 0 with value: 0.3097383240156475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.7876 Acc: 31.48% Kappa: 0.234\n",
      "Training complete in 4m 58s\n",
      "Best val Acc: 34.68%\n",
      "Epoch 0/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2110 Acc: 45.52% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3698 Acc: 35.55% Kappa: 0.288\n",
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1962 Acc: 47.44% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3646 Acc: 35.48% Kappa: 0.304\n",
      "Epoch 2/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1895 Acc: 48.89% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3644 Acc: 34.98% Kappa: 0.296\n",
      "Epoch 3/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1871 Acc: 49.05% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:06<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3636 Acc: 34.91% Kappa: 0.299\n",
      "Epoch 4/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:52<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1833 Acc: 49.57% Kappa: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.31it/s]\n",
      "[I 2024-06-19 10:48:24,776] Trial 2 finished with value: 0.30382318832937816 and parameters: {'epochs': 5, 'lr': 0.00019676675660946138, 'momentum': 0.2319990811141795}. Best is trial 0 with value: 0.3097383240156475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.3648 Acc: 35.55% Kappa: 0.303\n",
      "Training complete in 4m 59s\n",
      "Best val Acc: 35.48%\n",
      "Epoch 0/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/235 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize',\n",
    "                            storage=\"sqlite:///db.sqlite3\",  # Specify the storage URL here.\n",
    "                            study_name=f'{MODEL_NAME}_{MODEL_VERSION}',\n",
    "                            load_if_exists = True)\n",
    "study.optimize(optuna_train, n_trials=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp-tomi-2RV3BUvH-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
